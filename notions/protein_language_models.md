Les pLM (protein Language Models) considèrent les aa comme des tokens, et les séquences protéiques comme des phrases. 
- Etape 1 : entrainement auto-supervisé, c'est à dire en cachant et essayant de prédire des tokens cachés dans des séquences connues
- Etape 2 : extraction des embeddings obtenus et utilisation comme input pour des entrainements supervisés



